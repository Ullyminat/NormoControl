package checker

import (
	"academic-check-sys/internal/models"
	"encoding/json"
	"fmt"
	"math"
	"regexp"
	"strconv"
	"strings"
)

// CheckService orchestrates the check
type CheckService struct {
	Parser *DocParser
}

func NewCheckService() *CheckService {
	return &CheckService{
		Parser: NewDocParser(),
	}
}

// ConfigSchema defines what the frontend Standard JSON should look like
type ConfigSchema struct {
	Margins      MarginsConfig      `json:"margins"`
	Font         FontConfig         `json:"font"`
	Paragraph    ParagraphConfig    `json:"paragraph"`
	PageSetup    PageSetupConfig    `json:"page_setup"`
	HeaderFooter HeaderFooterConfig `json:"header_footer"` // New
	Typography   TypographyConfig   `json:"typography"`
	Structure    StructureConfig    `json:"structure"`
	Scope        ScopeConfig        `json:"scope"`        // New
	Introduction IntroductionConfig `json:"introduction"` // New
}

type IntroductionConfig struct {
	MinPages                   int  `json:"min_pages"`
	MaxPages                   int  `json:"max_pages"`
	VerifyPageCountDeclaration bool `json:"verify_page_count_declaration"` // New: Check if text matches real pages
}

type ScopeConfig struct {
	StartPage      int    `json:"start_page"`
	MinPages       int    `json:"min_pages"`
	MaxPages       int    `json:"max_pages"`
	ForbiddenWords string `json:"forbidden_words"` // Comma-sep list
}

type MarginsConfig struct {
	Top       float64 `json:"top"`
	Bottom    float64 `json:"bottom"`
	Left      float64 `json:"left"`
	Right     float64 `json:"right"`
	Tolerance float64 `json:"tolerance"`
}

type PageSetupConfig struct {
	Orientation string `json:"orientation"` // portrait, landscape
}

type HeaderFooterConfig struct {
	HeaderDist float64 `json:"header_dist"`
	FooterDist float64 `json:"footer_dist"`
}

type TypographyConfig struct {
	ForbidBold      bool `json:"forbid_bold"`
	ForbidItalic    bool `json:"forbid_italic"`
	ForbidUnderline bool `json:"forbid_underline"`
	ForbidAllCaps   bool `json:"forbid_all_caps"`
}

type StructureConfig struct {
	Heading1StartNewPage bool   `json:"heading_1_start_new_page"`
	HeadingHierarchy     bool   `json:"heading_hierarchy"`
	ListAlignment        string `json:"list_alignment"`
	VerifyTOC            bool   `json:"verify_toc"` // New: Check TOC page numbers
}

type FontConfig struct {
	Name string  `json:"name"`
	Size float64 `json:"size"`
}

type ParagraphConfig struct {
	LineSpacing     float64 `json:"line_spacing"`
	Alignment       string  `json:"alignment"`
	FirstLineIndent float64 `json:"first_line_indent"`
}

func (s *CheckService) RunCheck(filePath string, standardJSON string) (*models.CheckResult, []models.Violation, error) {
	// 1. Parse Document
	doc, err := s.Parser.Parse(filePath)
	if err != nil {
		return nil, nil, err
	}

	// 2. Parse Config
	var config ConfigSchema
	if err := json.Unmarshal([]byte(standardJSON), &config); err != nil {
		return nil, nil, fmt.Errorf("invalid standard config: %v", err)
	}

	// 3. Verify
	violations := []models.Violation{}
	totalRules := 0

	// Check Margins
	vListMargins := checkMargins(doc.Margins, config.Margins)
	totalRules += 4
	violations = append(violations, vListMargins...)

	// Check Page Setup
	if config.PageSetup.Orientation != "" && doc.PageSize.Orientation != "" {
		totalRules++
		if config.PageSetup.Orientation != doc.PageSize.Orientation {
			violations = append(violations, models.Violation{
				RuleType: "page_orientation", Description: "Incorrect Page Orientation",
				ExpectedValue: config.PageSetup.Orientation, ActualValue: doc.PageSize.Orientation, Severity: "error",
			})
		}
	}

	// Check Header/Footer
	if config.HeaderFooter.HeaderDist > 0 && math.Abs(doc.Margins.HeaderMm-config.HeaderFooter.HeaderDist) > 2.0 {
		totalRules++
		violations = append(violations, models.Violation{
			RuleType: "header_dist", Description: "Incorrect Header Distance", Severity: "error",
			ExpectedValue: fmt.Sprintf("%.1f mm", config.HeaderFooter.HeaderDist), ActualValue: fmt.Sprintf("%.1f mm", doc.Margins.HeaderMm),
		})
	} else if config.HeaderFooter.HeaderDist > 0 {
		totalRules++
	}

	if config.HeaderFooter.FooterDist > 0 && math.Abs(doc.Margins.FooterMm-config.HeaderFooter.FooterDist) > 2.0 {
		totalRules++
		violations = append(violations, models.Violation{
			RuleType: "footer_dist", Description: "Incorrect Footer Distance", Severity: "error",
			ExpectedValue: fmt.Sprintf("%.1f mm", config.HeaderFooter.FooterDist), ActualValue: fmt.Sprintf("%.1f mm", doc.Margins.FooterMm),
		})
	} else if config.HeaderFooter.FooterDist > 0 {
		totalRules++
	}

	// Check Paragraphs
	lastHeadingLevel := 0
	for i, p := range doc.Paragraphs {
		if strings.TrimSpace(p.Text) == "" {
			continue
		}

		// Page Scope Filter
		if config.Scope.StartPage > 1 && p.PageNumber < config.Scope.StartPage {
			// Skip checks for this paragraph as it is out of scope (e.g. title page)
			continue
		}

		// ID for Violation
		pos := fmt.Sprintf("Page %d, Para %d: %s...", p.PageNumber, i+1, truncate(p.Text, 100))

		isHeading := false
		headingLevel := 0
		if p.StyleID != "" && strings.Contains(strings.ToLower(p.StyleID), "heading") {
			isHeading = true
			if strings.Contains(p.StyleID, "1") {
				headingLevel = 1
			}
			if strings.Contains(p.StyleID, "2") {
				headingLevel = 2
			}
			if strings.Contains(p.StyleID, "3") {
				headingLevel = 3
			}
		}

		// Vocabulary Check will be done inside !isHeading block below

		// --- Structure Rules ---

		// 1. Heading 1 starts new page
		if config.Structure.Heading1StartNewPage && headingLevel == 1 {
			// Check if this Para has a page break, OR if the PREVIOUS para ending had one (simplified: we just check current para 'StartsPageBreak' flag from Parser)
			// Actually reliable detection needs to check previous run's break or this para's "pageBreakBefore" property (not yet fully parsed, but let's use what we have: Runs with Br)
			// Our parser flags 'StartsPageBreak' if it finds <w:br type="page"> in the runs.
			// Often "Page Break Before" is a PPr property <w:pageBreakBefore/>. We didn't parse that yet.
			// Let's rely on explicit Breaks for now.
			if i > 0 && !p.StartsPageBreak {
				// Also check if previous paragraph had a break at the end?
				// Simplified: Warn if no break found.
				violations = append(violations, models.Violation{
					RuleType: "structure_break", Description: "–ó–∞–≥–æ–ª–æ–≤–æ–∫ 1 —É—Ä–æ–≤–Ω—è –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã", PositionInDoc: pos,
					ExpectedValue: "–†–∞–∑—Ä—ã–≤ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", ActualValue: "–°–ø–ª–æ—à–Ω–æ–π —Ç–µ–∫—Å—Ç", Severity: "warning", // Warning because our detection is partial
				})
			}
		}

		// 2. Heading Hierarchy (1 -> 2 -> 3)
		if config.Structure.HeadingHierarchy && isHeading && headingLevel > 0 {
			if headingLevel > lastHeadingLevel+1 {
				violations = append(violations, models.Violation{
					RuleType: "structure_hierarchy", Description: fmt.Sprintf("–ü—Ä–æ–ø—É—â–µ–Ω —É—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–∞: H%d –ø–æ—Å–ª–µ H%d", headingLevel, lastHeadingLevel), PositionInDoc: pos,
					ExpectedValue: fmt.Sprintf("–ó–∞–≥–æ–ª–æ–≤–æ–∫ %d", lastHeadingLevel+1), ActualValue: fmt.Sprintf("–ó–∞–≥–æ–ª–æ–≤–æ–∫ %d", headingLevel), Severity: "error",
				})
			}
			lastHeadingLevel = headingLevel
		}
		if !isHeading {
			// Reset hierarchy check? No, body text doesn't reset level.
		}

		// --- TOC Verification ---
		if config.Structure.VerifyTOC && (strings.HasPrefix(strings.ToLower(p.StyleID), "toc") || strings.HasPrefix(strings.ToLower(p.StyleID), "table of contents")) {
			// Parse TOC Entry with improved regex support
			// Formats supported:
			// "Some Title ...... 5"
			// "Some Title    .    .    .    7"
			// "Some Title		10"  (tabs)
			// "1. Chapter Title ......... 12"
			text := strings.TrimSpace(p.Text)

			// Skip empty or very short TOC entries
			if len(text) < 3 {
				continue
			}

			// Enhanced regex pattern to extract title and page number
			// Matches: "Title [dots/spaces/tabs] PageNumber"
			// Captures: 1=title, 2=page number
			tocPattern := `^(.+?)[\s\.\_\-]+(\d+)$`
			re := regexp.MustCompile(tocPattern)
			matches := re.FindStringSubmatch(text)

			if len(matches) >= 3 {
				titlePart := strings.TrimSpace(matches[1])
				pagePart := matches[2]

				// Clean up title: remove trailing dots, underscores, dashes, spaces
				titlePart = strings.TrimRight(titlePart, " ._-")

				if tocPage, err := strconv.Atoi(pagePart); err == nil {
					// Found a valid TOC entry structure. Now find the heading.
					// Search whole doc for this heading
					// Build a map of headings for O(1) lookup (optimization)
					if len(doc.Paragraphs) > 100 {
						// For large docs, use map
						headingMap := make(map[string]int)
						for _, targetP := range doc.Paragraphs {
							if targetP.StyleID != "" && strings.Contains(strings.ToLower(targetP.StyleID), "heading") {
								normalizedTitle := strings.ToLower(strings.TrimSpace(targetP.Text))
								headingMap[normalizedTitle] = targetP.PageNumber
							}
						}

						normalizedSearchTitle := strings.ToLower(titlePart)
						if actualPage, found := headingMap[normalizedSearchTitle]; found {
							if actualPage != tocPage {
								violations = append(violations, models.Violation{
									RuleType: "toc_page_mismatch", Description: fmt.Sprintf("–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–∏ –¥–ª—è '%s'", truncate(titlePart, 20)), PositionInDoc: "–û–≥–ª–∞–≤–ª–µ–Ω–∏–µ",
									ExpectedValue: fmt.Sprintf("–°—Ç—Ä. %d", actualPage), ActualValue: fmt.Sprintf("–°—Ç—Ä. %d", tocPage), Severity: "error",
								})
							}
						}
					} else {
						// For small docs, linear search is fine
						for _, targetP := range doc.Paragraphs {
							if targetP.StyleID != "" && strings.Contains(strings.ToLower(targetP.StyleID), "heading") {
								// Compare Text with case-insensitive trim
								if strings.EqualFold(strings.TrimSpace(targetP.Text), titlePart) {
									if targetP.PageNumber != tocPage {
										violations = append(violations, models.Violation{
											RuleType: "toc_page_mismatch", Description: fmt.Sprintf("–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–∏ –¥–ª—è '%s'", truncate(titlePart, 20)), PositionInDoc: "–û–≥–ª–∞–≤–ª–µ–Ω–∏–µ",
											ExpectedValue: fmt.Sprintf("–°—Ç—Ä. %d", targetP.PageNumber), ActualValue: fmt.Sprintf("–°—Ç—Ä. %d", tocPage), Severity: "error",
										})
									}
									break
								}
							}
						}
						// Note: We don't warn if heading not found to avoid false positives
					}
				}
			}
		}

		// --- Formatting Rules (Skip for Headings usually, but user might want strictness) ---
		// We usually apply "Body" rules only to normal paragraphs (no style or Normal)

		if !isHeading {
			// --- Vocabulary Check (only for body text, not headings) ---
			if config.Scope.ForbiddenWords != "" {
				words := strings.Split(config.Scope.ForbiddenWords, ",")
				lowerText := strings.ToLower(p.Text)
				for _, w := range words {
					w = strings.TrimSpace(strings.ToLower(w))
					if w != "" && strings.Contains(lowerText, w) {
						violations = append(violations, models.Violation{
							RuleType: "vocabulary", Description: fmt.Sprintf("–ó–∞–ø—Ä–µ—â–µ–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞: '%s'", w), PositionInDoc: pos,
							ExpectedValue: "–ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å", ActualValue: "–ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç", Severity: "error",
							ContextText: p.Text,
						})
					}
				}
			}

			// Font Check
			if p.FontName != "" && config.Font.Name != "" {
				totalRules++
				if p.FontName != config.Font.Name {
					violations = append(violations, models.Violation{
						RuleType: "font_name", Description: "–ù–µ–≤–µ—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç", PositionInDoc: pos,
						ExpectedValue: config.Font.Name, ActualValue: p.FontName, Severity: "error",
						ContextText: p.Text,
					})
				}
			}
			if p.FontSizePt > 0 && config.Font.Size > 0 {
				totalRules++
				if math.Abs(p.FontSizePt-config.Font.Size) > 0.5 {
					violations = append(violations, models.Violation{
						RuleType: "font_size", Description: "–ù–µ–≤–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞", PositionInDoc: pos,
						ExpectedValue: fmt.Sprintf("%.1f", config.Font.Size), ActualValue: fmt.Sprintf("%.1f", p.FontSizePt), Severity: "error",
						ContextText: p.Text,
					})
				}
			}

			// Spacing
			if config.Paragraph.LineSpacing > 0 {
				totalRules++
				if math.Abs(p.LineSpacing-config.Paragraph.LineSpacing) > 0.1 {
					violations = append(violations, models.Violation{
						RuleType: "line_spacing", Description: "–ù–µ–≤–µ—Ä–Ω—ã–π –º–µ–∂–¥—É—Å—Ç—Ä–æ—á–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª", PositionInDoc: pos,
						ExpectedValue: fmt.Sprintf("%.1f", config.Paragraph.LineSpacing), ActualValue: fmt.Sprintf("%.1f", p.LineSpacing), Severity: "warning",
						ContextText: p.Text,
					})
				}
			}

			// Justification
			expectedAlign := config.Paragraph.Alignment
			if expectedAlign != "" {
				totalRules++
				if expectedAlign == "justify" && p.Alignment != "both" {
					violations = append(violations, models.Violation{
						RuleType: "alignment", Description: "–ù–µ–≤–µ—Ä–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ", PositionInDoc: pos,
						ExpectedValue: "–ø–æ —à–∏—Ä–∏–Ω–µ", ActualValue: p.Alignment, Severity: "warning",
						ContextText: p.Text,
					})
				}
			}

			// Indentation
			if config.Paragraph.FirstLineIndent > 0 {
				totalRules++
				if math.Abs(p.FirstLineIndentMm-config.Paragraph.FirstLineIndent) > 2.0 {
					violations = append(violations, models.Violation{
						RuleType: "indent", Description: "–ù–µ–≤–µ—Ä–Ω—ã–π –æ—Ç—Å—Ç—É–ø –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏", PositionInDoc: pos,
						ExpectedValue: fmt.Sprintf("%.1f", config.Paragraph.FirstLineIndent), ActualValue: fmt.Sprintf("%.1f", p.FirstLineIndentMm), Severity: "warning",
						ContextText: p.Text,
					})
				}
			}

			// Advanced Typography Controls
			if config.Typography.ForbidBold {
				totalRules++
				if p.IsBold {
					violations = append(violations, models.Violation{
						RuleType: "style_bold", Description: "–ñ–∏—Ä–Ω—ã–π —à—Ä–∏—Ñ—Ç –∑–∞–ø—Ä–µ—â–µ–Ω –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ç–µ–∫—Å—Ç–µ", PositionInDoc: pos,
						ExpectedValue: "–û–±—ã—á–Ω—ã–π", ActualValue: "–ñ–∏—Ä–Ω—ã–π", Severity: "error",
						ContextText: p.Text,
					})
				}
			}
			if config.Typography.ForbidItalic {
				totalRules++
				if p.IsItalic {
					violations = append(violations, models.Violation{
						RuleType: "style_italic", Description: "–ö—É—Ä—Å–∏–≤ –∑–∞–ø—Ä–µ—â–µ–Ω –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ç–µ–∫—Å—Ç–µ", PositionInDoc: pos,
						ExpectedValue: "–û–±—ã—á–Ω—ã–π", ActualValue: "–ö—É—Ä—Å–∏–≤", Severity: "error",
						ContextText: p.Text,
					})
				}
			}
			if config.Typography.ForbidUnderline {
				totalRules++
				if p.IsUnderline {
					violations = append(violations, models.Violation{
						RuleType: "style_underline", Description: "–ü–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–µ—â–µ–Ω–æ", PositionInDoc: pos,
						ExpectedValue: "–û–±—ã—á–Ω—ã–π", ActualValue: "–ü–æ–¥—á–µ—Ä–∫–Ω—É—Ç—ã–π", Severity: "error",
						ContextText: p.Text,
					})
				}
			}
			if config.Typography.ForbidAllCaps {
				totalRules++
				if p.IsAllCaps {
					violations = append(violations, models.Violation{
						RuleType: "style_caps", Description: "–í–°–ï –ó–ê–ì–õ–ê–í–ù–´–ï –∑–∞–ø—Ä–µ—â–µ–Ω—ã", PositionInDoc: pos,
						ExpectedValue: "–û–±—ã—á–Ω—ã–π", ActualValue: "–í–°–ï –ó–ê–ì–õ–ê–í–ù–´–ï", Severity: "error",
						ContextText: p.Text,
					})
				}
			}
		}
	}

	// Check Doc Limits
	if config.Scope.MinPages > 0 && doc.Stats.TotalPages < config.Scope.MinPages {
		violations = append(violations, models.Violation{
			RuleType: "doc_length", Description: "–î–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π", PositionInDoc: "–ì–ª–æ–±–∞–ª—å–Ω–æ",
			ExpectedValue: fmt.Sprintf("–ú–∏–Ω. %d —Å—Ç—Ä.", config.Scope.MinPages), ActualValue: fmt.Sprintf("%d —Å—Ç—Ä.", doc.Stats.TotalPages), Severity: "error",
		})
	}
	if config.Scope.MaxPages > 0 && doc.Stats.TotalPages > config.Scope.MaxPages {
		violations = append(violations, models.Violation{
			RuleType: "doc_length", Description: "–î–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π", PositionInDoc: "–ì–ª–æ–±–∞–ª—å–Ω–æ",
			ExpectedValue: fmt.Sprintf("–ú–∞–∫—Å. %d —Å—Ç—Ä.", config.Scope.MaxPages), ActualValue: fmt.Sprintf("%d —Å—Ç—Ä.", doc.Stats.TotalPages), Severity: "error",
		})
	}

	// Check Introduction Pages
	if config.Introduction.MinPages > 0 || config.Introduction.MaxPages > 0 || config.Introduction.VerifyPageCountDeclaration {
		startPage := -1
		endPage := -1
		var introductionText strings.Builder // Collect all intro text for declaration check

		for _, p := range doc.Paragraphs {
			if p.StyleID != "" && strings.Contains(strings.ToLower(p.StyleID), "heading") {
				// Simple heuristic for Introduction heading
				text := strings.ToLower(strings.TrimSpace(p.Text))
				if startPage == -1 && (strings.Contains(text, "–≤–≤–µ–¥–µ–Ω–∏–µ") || strings.Contains(text, "introduction")) {
					startPage = p.PageNumber
				} else if startPage != -1 && endPage == -1 {
					// Next heading marks the end
					endPage = p.PageNumber
					break
				}
			}

			// Collect intro text for declaration verification
			if startPage != -1 && endPage == -1 {
				introductionText.WriteString(p.Text)
				introductionText.WriteString(" ")
			}
		}

		// If endPage is not found but startPage is found, assume it goes to the end of document
		if startPage != -1 && endPage == -1 {
			endPage = doc.Stats.TotalPages
			// If total pages is the same as start page, we still count as 1
			if endPage < startPage {
				endPage = startPage
			}
		}

		if startPage != -1 {
			// Correct calculation: if intro starts at page 5 and next section at page 8,
			// intro occupies pages 5,6,7 = 3 pages (endPage - startPage)
			// But if intro is alone until end, we need +1
			pCount := endPage - startPage
			if pCount == 0 {
				pCount = 1
			}

			if config.Introduction.MinPages > 0 && pCount < config.Introduction.MinPages {
				violations = append(violations, models.Violation{
					RuleType: "intro_length", Description: "–í–≤–µ–¥–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ", PositionInDoc: fmt.Sprintf("–°—Ç—Ä. %d-%d", startPage, endPage),
					ExpectedValue: fmt.Sprintf("–ú–∏–Ω. %d —Å—Ç—Ä.", config.Introduction.MinPages), ActualValue: fmt.Sprintf("%d —Å—Ç—Ä.", pCount), Severity: "error",
				})
			}
			if config.Introduction.MaxPages > 0 && pCount > config.Introduction.MaxPages {
				violations = append(violations, models.Violation{
					RuleType: "intro_length", Description: "–í–≤–µ–¥–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ", PositionInDoc: fmt.Sprintf("–°—Ç—Ä. %d-%d", startPage, endPage),
					ExpectedValue: fmt.Sprintf("–ú–∞–∫—Å. %d —Å—Ç—Ä.", config.Introduction.MaxPages), ActualValue: fmt.Sprintf("%d —Å—Ç—Ä.", pCount), Severity: "error",
				})
			}

			// NEW: Verify page count declaration if enabled
			if config.Introduction.VerifyPageCountDeclaration {
				// Look for patterns like:
				// "–í–≤–µ–¥–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
				// "–¥–∞–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª –∑–∞–Ω–∏–º–∞–µ—Ç 2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
				// "Introduction spans 4 pages"
				introText := strings.ToLower(introductionText.String())

				// Regex patterns to find declared page counts
				// Russian: "—Å–æ–¥–µ—Ä–∂–∏—Ç X —Å—Ç—Ä–∞–Ω–∏—Ü", "–∑–∞–Ω–∏–º–∞–µ—Ç X —Å—Ç—Ä–∞–Ω–∏—Ü"
				// English: "contains X pages", "spans X pages"
				patterns := []string{
					`—Å–æ–¥–µ—Ä–∂–∏—Ç\s+(\d+)\s+—Å—Ç—Ä–∞–Ω–∏—Ü`,
					`–∑–∞–Ω–∏–º–∞–µ—Ç\s+(\d+)\s+—Å—Ç—Ä–∞–Ω–∏—Ü`,
					`—Å–æ–¥–µ—Ä–∂–∏—Ç\s+(\d+)\s+—Å—Ç—Ä`,
					`–∑–∞–Ω–∏–º–∞–µ—Ç\s+(\d+)\s+—Å—Ç—Ä`,
					`contains\s+(\d+)\s+pages?`,
					`spans\s+(\d+)\s+pages?`,
				}

				declaredPages := -1

				for _, pattern := range patterns {
					re := regexp.MustCompile(pattern)
					matches := re.FindStringSubmatch(introText)
					if len(matches) > 1 {
						// Found a match, extract the number
						if num, err := strconv.Atoi(matches[1]); err == nil {
							declaredPages = num
							break
						}
					}
				}

				// If we found a declaration, verify it
				if declaredPages > 0 && declaredPages != pCount {
					violations = append(violations, models.Violation{
						RuleType:      "intro_page_declaration_mismatch",
						Description:   "–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∑–∞—è–≤–ª–µ–Ω–Ω–æ–≥–æ –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –í–≤–µ–¥–µ–Ω–∏—è",
						PositionInDoc: fmt.Sprintf("–í–≤–µ–¥–µ–Ω–∏–µ (–°—Ç—Ä. %d-%d)", startPage, endPage),
						ExpectedValue: fmt.Sprintf("–§–∞–∫—Ç–∏—á–µ—Å–∫–∏: %d —Å—Ç—Ä.", pCount),
						ActualValue:   fmt.Sprintf("–ó–∞—è–≤–ª–µ–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ: %d —Å—Ç—Ä.", declaredPages),
						Severity:      "warning", // Warning, not error, as declaration might be optional
						ContextText:   truncate(introductionText.String(), 200),
					})
				}
			}
		}
	}

	// Calculate Score
	// Proper formula: score = (passed / total) * 100
	passedRules := totalRules - len(violations)
	if passedRules < 0 {
		passedRules = 0
	}

	score := 0.0
	if totalRules > 0 {
		score = math.Max(0, (float64(passedRules)/float64(totalRules))*100.0)
	}

	res := &models.CheckResult{
		OverallScore: score,
		TotalRules:   totalRules,
		FailedRules:  len(violations),
		PassedRules:  passedRules,
	}

	fmt.Printf("üìä Checker: TotalRules=%d, Violations=%d, PassedRules=%d, Score=%.2f\n", totalRules, len(violations), passedRules, score)

	// Serialize Content for View
	if contentBytes, err := json.Marshal(doc); err == nil {
		res.ContentJSON = string(contentBytes)
	}

	return res, violations, nil
}

func checkMargins(actual Margins, target MarginsConfig) []models.Violation {
	vs := []models.Violation{}
	tol := target.Tolerance
	if tol == 0 {
		tol = 2.0
	} // Default 2mm tolerance

	if math.Abs(actual.TopMm-target.Top) > tol {
		vs = append(vs, models.Violation{RuleType: "margin_top", Description: "–ù–µ–≤–µ—Ä–Ω—ã–π –≤–µ—Ä—Ö–Ω–∏–π –æ—Ç—Å—Ç—É–ø", Severity: "error", ExpectedValue: fmt.Sprintf("%.1f –º–º", target.Top), ActualValue: fmt.Sprintf("%.1f –º–º", actual.TopMm)})
	}
	if math.Abs(actual.BottomMm-target.Bottom) > tol {
		vs = append(vs, models.Violation{RuleType: "margin_bottom", Description: "–ù–µ–≤–µ—Ä–Ω—ã–π –Ω–∏–∂–Ω–∏–π –æ—Ç—Å—Ç—É–ø", Severity: "error", ExpectedValue: fmt.Sprintf("%.1f –º–º", target.Bottom), ActualValue: fmt.Sprintf("%.1f –º–º", actual.BottomMm)})
	}
	if math.Abs(actual.LeftMm-target.Left) > tol {
		vs = append(vs, models.Violation{RuleType: "margin_left", Description: "–ù–µ–≤–µ—Ä–Ω—ã–π –ª–µ–≤—ã–π –æ—Ç—Å—Ç—É–ø", Severity: "error", ExpectedValue: fmt.Sprintf("%.1f –º–º", target.Left), ActualValue: fmt.Sprintf("%.1f –º–º", actual.LeftMm)})
	}
	if math.Abs(actual.RightMm-target.Right) > tol {
		vs = append(vs, models.Violation{RuleType: "margin_right", Description: "–ù–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–∞–≤—ã–π –æ—Ç—Å—Ç—É–ø", Severity: "error", ExpectedValue: fmt.Sprintf("%.1f –º–º", target.Right), ActualValue: fmt.Sprintf("%.1f –º–º", actual.RightMm)})
	}
	return vs
}

func truncate(s string, n int) string {
	if len(s) > n {
		return s[:n]
	}
	return s
}
